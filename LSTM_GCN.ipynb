{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70326995",
   "metadata": {},
   "source": [
    "## Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de66be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1,2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34041ee1",
   "metadata": {},
   "source": [
    "## load the position dictionary to locate where to put the code embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dxdict=pickle.load(open('dxpos.pkl','rb'))\n",
    "meddict=pickle.load(open('medpos.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff806807",
   "metadata": {},
   "source": [
    "## Load the data, including the doc2vec embedding, bert embedding and raw feature input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6089e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "## Raw data feature input\n",
    "optrainx=np.load('optrainx.npy')\n",
    "optrainy=np.load('optrainy.npy')\n",
    "\n",
    "## BERT embedding for clinical code\n",
    "token2emb=np.load('token2emb_bert.npy')\n",
    "## the edge data, [0] [1] are the connected nodes\n",
    "opedges=np.load('opedges.npy')\n",
    "\n",
    "## Doc2vec embedding for clinical code\n",
    "diagnosis_desc_dict=pickle.load(open('diagnosis_desc_dict.pkl','rb'))\n",
    "med_desc_dict=pickle.load(open('med_desc_dict.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3140fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of patients\n",
    "patient_totest=len(optrainx)\n",
    "\n",
    "## number of clinical codes that have embeddings\n",
    "num_codes=len(diagnosis_desc_dict)+len(med_desc_dict)\n",
    "\n",
    "\n",
    "## use embedding to initialize the node features\n",
    "embedding_layer = tf.keras.layers.Embedding(num_codes, 64)\n",
    "\n",
    "## node feature vector for clinical code\n",
    "opnode_features_part_c=embedding_layer(np.asarray([i for i in range(num_codes)])).numpy()\n",
    "## node feature vector for encounter and patient\n",
    "opnode_features_part_ep = np.random.rand(patient_totest,32)\n",
    "\n",
    "## replace the clinical code embeddings by bert and doc2vec embeddings\n",
    "for dx in diagnosis_desc_dict:\n",
    "    opnode_features_part_c[dxdict[dx],:32]=diagnosis_desc_dict[dx]\n",
    "\n",
    "for med in med_desc_dict:\n",
    "    opnode_features_part_c[meddict[med]+414,:32]=med_desc_dict[med]\n",
    "\n",
    "for i in range(562):\n",
    "    opnode_features_part_c[i,32:]=token2emb[i]\n",
    "\n",
    "print(opnode_features_part_c.shape,opnode_features_part_ep.shape)\n",
    "\n",
    "\n",
    "opedges=tf.convert_to_tensor(opedges,dtype=tf.int64)\n",
    "\n",
    "graph_info = (opnode_features_part_c, opnode_features_part_ep, opedges)\n",
    "\n",
    "other_features=optrainx[:patient_totest,:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eccb2c",
   "metadata": {},
   "source": [
    "## Build the input x and y, then split the train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opx=[]\n",
    "opy=[]\n",
    "\n",
    "for i in range(patient_totest):\n",
    "    opx.append([j for j in range(num_codes+i*5,num_codes+5+i*5)])\n",
    "\n",
    "opx=np.asarray(opx)\n",
    "\n",
    "opy=optrainy[:patient_totest]\n",
    "\n",
    "\n",
    "trainopx1=opx[:int(patient_totest*0.8)]\n",
    "trainopx2=optrainx[:int(patient_totest*0.8)]\n",
    "trainopy=opy[:int(patient_totest*0.8)]\n",
    "\n",
    "valopx1=opx[int(patient_totest*0.8):]\n",
    "valopx2=optrainx[int(patient_totest*0.8):]\n",
    "valopy=opy[int(patient_totest*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d05d315",
   "metadata": {},
   "source": [
    "## Implement Feedforward Network (FFN) Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eefbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.relu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca5e05",
   "metadata": {},
   "source": [
    "## Build a Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4243003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGraphConvLayer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_units,\n",
    "        dropout_rate=0.2,\n",
    "        aggregation_type=\"mean\",\n",
    "        combination_type=\"add\",\n",
    "        normalize=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.combination_type = combination_type\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
    "        self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
    "\n",
    "    def prepare(self, node_repesentations, weights=None):\n",
    "        # node_repesentations shape is [num_edges, embedding_dim].\n",
    "        messages = self.ffn_prepare(node_repesentations)\n",
    "        #if weights is not None:\n",
    "        #    messages = messages * tf.expand_dims(weights, -1)\n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages, node_repesentations):\n",
    "        # node_indices shape is [num_edges].\n",
    "        # neighbour_messages shape: [num_edges, representation_dim].\n",
    "        # node_repesentations shape is [num_nodes, representation_dim]\n",
    "        num_nodes = node_repesentations.shape[0]\n",
    "\n",
    "        aggregated_message = tf.math.unsorted_segment_mean(\n",
    "                neighbour_messages, node_indices, num_segments=num_nodes\n",
    "        )\n",
    "\n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_repesentations, aggregated_messages):\n",
    "        # node_repesentations shape is [num_nodes, representation_dim].\n",
    "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
    "        # Add node_repesentations and aggregated_messages.\n",
    "        h = node_repesentations + aggregated_messages\n",
    "        # Apply the processing function.\n",
    "        node_embeddings = self.update_fn(h)\n",
    "        if self.combination_type == \"gru\":\n",
    "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
    "\n",
    "        if self.normalize:\n",
    "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
    "        return node_embeddings\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        node_repesentations, edges = inputs\n",
    "        # Get node_indices (source) and neighbour_indices (target) from edges.\n",
    "        node_indices, neighbour_indices = edges[0], edges[1]\n",
    "        # neighbour_repesentations shape is [num_edges, representation_dim].\n",
    "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
    "        \n",
    "        # Prepare the messages of the neighbours.\n",
    "        neighbour_messages = self.prepare(neighbour_repesentations)\n",
    "        # Aggregate the neighbour messages.\n",
    "        aggregated_messages = self.aggregate(\n",
    "            node_indices, neighbour_messages, node_repesentations\n",
    "        )\n",
    "        # Update the node embedding with the neighbour messages.\n",
    "        return self.update(node_repesentations, aggregated_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c8c56f",
   "metadata": {},
   "source": [
    "## To implement LSTM + GNN integrated model with nodes encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96aa036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGNNClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph_info,\n",
    "        hidden_units,\n",
    "        aggregation_type=\"sum\",\n",
    "        combination_type=\"concat\",\n",
    "        dropout_rate=0.2,\n",
    "        normalize=True,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
    "        clinical_features, enc_pat_features, edges = graph_info\n",
    "        self.clinical_features = clinical_features\n",
    "        self.enc_pat_features = enc_pat_features\n",
    "        self.edges = edges\n",
    "\n",
    "        # Create a process layer.\n",
    "        self.clinical_encoder = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
    "        # Create the first GraphConv layer.\n",
    "        self.conv1 = HeteroGraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv1\",\n",
    "        )\n",
    "        \n",
    "        self.conv2 = HeteroGraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv2\",\n",
    "        )\n",
    "        \n",
    "        self.conv3 = HeteroGraphConvLayer(\n",
    "            hidden_units,\n",
    "            dropout_rate,\n",
    "            aggregation_type,\n",
    "            combination_type,\n",
    "            normalize,\n",
    "            name=\"graph_conv3\",\n",
    "        )\n",
    "        \n",
    "        # Create a postprocess layer.\n",
    "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
    "        \n",
    "        self.lstm_1= tf.keras.layers.LSTM(16,return_sequences=True)\n",
    "        self.lstm_2= tf.keras.layers.LSTM(16)\n",
    "\n",
    "        self.lastlayer = layers.Dense(units=8, activation='relu')\n",
    "        # Create a compute logits layer.\n",
    "        self.finaloutput = layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Preprocess the node_features to produce node representations.\n",
    "        \n",
    "        input_node_indices,otherfeatures=inputs[0],inputs[1]\n",
    "        \n",
    "        xc = self.clinical_encoder(self.clinical_features)\n",
    "        xep = self.preprocess(self.enc_pat_features)\n",
    "        \n",
    "        x = tf.concat((xc,xep),axis=0)\n",
    "        \n",
    "        # Apply the first graph conv layer.\n",
    "        x1 = self.conv1((x, self.edges))\n",
    "\n",
    "        x = x1 + x\n",
    "        \n",
    "        opedges2=tf.gather(self.edges,[1,0],axis=0)\n",
    "        \n",
    "        # Apply the second graph conv layer.\n",
    "        x2 = self.conv2((x, opedges2))\n",
    "        # Skip connection.\n",
    "        x = x2 + x\n",
    "        \n",
    "        # Postprocess node embedding.\n",
    "        # Apply the second graph conv layer.\n",
    "        x3 = self.conv3((x, self.edges))\n",
    "        # Skip connection.\n",
    "        x = x3 + x\n",
    "        # Postprocess node embedding.\n",
    "        x = self.postprocess(x)\n",
    "\n",
    "        # Fetch node embeddings for the input node_indices.\n",
    "        node_embeddings = tf.gather(x, input_node_indices)\n",
    "        #step_embeddings=node_embeddings.reshape((input_node_indices.shape[0],5,node_embeddings.shape[-1]))\n",
    "        concat_embeddings=tf.concat([node_embeddings, otherfeatures], 2)\n",
    "        lstm_embeddings_1=self.lstm_1(concat_embeddings)\n",
    "        lstm_embeddings_2=self.lstm_2(lstm_embeddings_1)\n",
    "        output_embeddings=self.lastlayer(lstm_embeddings_2)\n",
    "        # Compute logits\n",
    "        \n",
    "        return self.finaloutput(output_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41829e88",
   "metadata": {},
   "source": [
    "\n",
    "## To initialize the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23269e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = [32, 32]\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.5\n",
    "\n",
    "\n",
    "gnn_model = LSTMGNNEncounterClassifier(\n",
    "    graph_info=graph_info,\n",
    "    hidden_units=hidden_units,\n",
    "    dropout_rate=dropout_rate,\n",
    "    name=\"gnnlstm_model\",\n",
    ")\n",
    "gnn_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"binary_accuracy\")],\n",
    ")\n",
    "\n",
    "gnn_model.fit([trainopx1,trainopx2],trainopy,epochs=50,batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121623c7",
   "metadata": {},
   "source": [
    "## Calculate the best f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a539156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rod(a,b):\n",
    "    if a>b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def bestf(predy,testy):\n",
    "    from sklearn.metrics import recall_score,precision_score,f1_score,roc_auc_score,average_precision_score\n",
    "    f1s=[]\n",
    "    for i in range(1,100):\n",
    "        j=i/100.0\n",
    "        pren=list(map(lambda x:rod(x,j),predy))\n",
    "        f1s.append([f1_score(testy,pren),precision_score(testy,pren),recall_score(testy,pren),i])\n",
    "    \n",
    "    maxf1=max(f1s)\n",
    "    auroc=roc_auc_score(testy,predy)\n",
    "    auprc=average_precision_score(testy,predy)\n",
    "    \n",
    "    print('F1 Score:',maxf1[0],' Precision Score:',maxf1[1],' Recall Score:',maxf1[2],'AUROC:',auroc,'Average Precision',auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestf(allprevaly,valopy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1753e18",
   "metadata": {},
   "source": [
    "## To test the validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d800b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allprevaly=[]\n",
    "for i in range(10):\n",
    "    prevaly=gnn_model.predict([valopx1[i*1000:(i+1)*1000],valopx2[i*1000:(i+1)*1000]])\n",
    "    allprevaly.append(prevaly)\n",
    "\n",
    "allprevaly=np.concatenate(allprevaly)\n",
    "bestf(allprevaly,valopy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
